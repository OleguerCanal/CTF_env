behaviors:
  CTFAgentBehavior:
    trainer_type: ppo
    hyperparameters:
      batch_size: 50  # Num experiences in each iteration of gradient descend
      buffer_size: 1000  # Num experiences to be filled before updating weights
      learning_rate: 3.0e-4
      beta: 5.0e-4  # Weight of the entropy: If entropy drops too quickly, increase beta
      epsilon: 0.2  # How rapidly can the policy change
      lambd: 0.99  #
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
      memory:
        memory_size: 100
        sequence_length: 50
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 1e6 # Steps collected in one training per-behavior
    time_horizon: 500 # Steps collected per-agent added to buffer
    summary_freq: 10000


environment_parameters:
  # Reward shaping
  finishReward: 1.0
  collectableReward: 0.5
  timeReward: -0.002  # Doing nothing leads to reward -1

  # Analytics logging
  # logPath: "Assets/Resources/completionist2.txt"
  logFreq: 10